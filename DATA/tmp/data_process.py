'''
# ä½¿ç”¨FinBertæ¨¡å‹åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡æ–°æ ¼å¼è™•ç† (å·²æ›´æ–°)
# ==============================================================================
def load_and_merge_data(path_pattern=None):
    """
    è¼‰å…¥æ‰€æœ‰ç¬¦åˆæ¨£å¼çš„ã€å·²æ¸…æ´—éçš„ JSON æª”æ¡ˆï¼Œä¸¦å¾å·¢ç‹€çµæ§‹ä¸­æå–æ‰€éœ€è³‡æ–™ã€‚
    å¦‚æœæœªæä¾› `path_pattern`ï¼Œæœƒè‡ªå‹•å¾ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„ '../processed' è³‡æ–™å¤¾å°‹æ‰¾ã€‚
    """
    if path_pattern is None:
        # æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª”æ¡ˆçµæ§‹ï¼Œè…³æœ¬åœ¨ 'DATA/tmp'ï¼Œè³‡æ–™åœ¨ 'DATA/processed'ã€‚
        # æˆ‘å€‘å°‡åŸºæ–¼è…³æœ¬çš„çµ•å°è·¯å¾‘ä¾†å»ºæ§‹è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"æœªæŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")

    json_files = glob.glob(path_pattern)
    if not json_files:
        # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•—ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
        searched_path = os.path.dirname(path_pattern)
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{searched_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„ 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'DATA/processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ä½¿ç”¨ pandas.json_normalize ä¾†é«˜æ•ˆåœ°è™•ç†å·¢ç‹€JSON
    # å¾ 'cleaned_data' å’Œ 'original_metadata' ä¸­æå–æˆ‘å€‘éœ€è¦çš„æ¬„ä½
    try:
        cleaned_data_df = pd.json_normalize(df['cleaned_data'])
        original_metadata_df = pd.json_normalize(df['original_metadata'])
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    # çµ„åˆä¸€å€‹ä¹¾æ·¨ã€æ‰å¹³åŒ–çš„ DataFrame
    final_df = pd.DataFrame({
        'id': df['original_tweet_id'].astype(str),
        'cleaned_text': cleaned_data_df['cleaned_text'],
        'createdAt': pd.to_datetime(original_metadata_df['createdAt']),
        'original_data': df['original_metadata'] # ä¿ç•™å®Œæ•´çš„åŸå§‹è³‡æ–™ä»¥ä¾›é–±è®€çª—æ ¼ä½¿ç”¨
    })
    
    # ç§»é™¤ 'cleaned_text' ç‚ºç©ºå€¼çš„è¡Œï¼Œä»¥é¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(final_df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ (å·²æ¿¾é™¤ç©ºå…§å®¹èˆ‡é‡è¤‡é …)ã€‚")
    if len(final_df) < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - len(final_df)} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    return final_df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—å‡½æ•¸ (æ­¤å‡½æ•¸å·²ä¸å†éœ€è¦ï¼Œæ•…åˆªé™¤)
# ==============================================================================
# def clean_text(text):  <-- æ­¤å‡½æ•¸å·²è¢«ç§»é™¤

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        X = vectorizer.transform(group['cleaned_text'])
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡ FinBERTa æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT)
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA)
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC)
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # D. æƒ…ç·’åˆ†æ (ä½¿ç”¨ FinBERTa)
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ (ä½¿ç”¨ FinBERTa æ¨¡å‹)...")
    sentiment_pipeline = pipeline("sentiment-analysis", model="ProsusAI/finbert")
    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    sentiment_map = {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'}
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # æº–å‚™è¼¸å‡ºçµæœ
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ (å·²æ›´æ–°)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘åœ¨è®€å–è³‡æ–™æ™‚ä¿ç•™çš„ 'original_data' æ¬„ä½
    # å°‡å…¶è½‰æ›ç‚ºä»¥ 'id' ç‚º key çš„å­—å…¸
    reading_pane_dict = pd.Series(df.original_data.values, index=df.id).to_dict()
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK (å·²æ›´æ–°)
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥ä¸¦è™•ç†æ–°æ ¼å¼çš„è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ–‡æœ¬æ¸…æ´—æ­¥é©Ÿå·²è¢«ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ç›´æ¥ä½¿ç”¨æª”æ¡ˆä¸­çš„ 'cleaned_text'
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")
'''








# ä½¿ç”¨RoBertæ¨¡å‹åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡æ–°æ ¼å¼è™•ç† (å·²æ›´æ–°)
# ==============================================================================
def load_and_merge_data(path_pattern=None):
    """
    è¼‰å…¥æ‰€æœ‰ç¬¦åˆæ¨£å¼çš„ã€å·²æ¸…æ´—éçš„ JSON æª”æ¡ˆï¼Œä¸¦å¾å·¢ç‹€çµæ§‹ä¸­æå–æ‰€éœ€è³‡æ–™ã€‚
    å¦‚æœæœªæä¾› `path_pattern`ï¼Œæœƒè‡ªå‹•å¾ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„ '../processed' è³‡æ–™å¤¾å°‹æ‰¾ã€‚
    """
    if path_pattern is None:
        # æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª”æ¡ˆçµæ§‹ï¼Œè…³æœ¬åœ¨ 'DATA/tmp'ï¼Œè³‡æ–™åœ¨ 'DATA/processed'ã€‚
        # æˆ‘å€‘å°‡åŸºæ–¼è…³æœ¬çš„çµ•å°è·¯å¾‘ä¾†å»ºæ§‹è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"æœªæŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")

    json_files = glob.glob(path_pattern)
    if not json_files:
        # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•—ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
        searched_path = os.path.dirname(path_pattern)
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{searched_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„ 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'DATA/processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ä½¿ç”¨ pandas.json_normalize ä¾†é«˜æ•ˆåœ°è™•ç†å·¢ç‹€JSON
    # å¾ 'cleaned_data' å’Œ 'original_metadata' ä¸­æå–æˆ‘å€‘éœ€è¦çš„æ¬„ä½
    try:
        cleaned_data_df = pd.json_normalize(df['cleaned_data'])
        original_metadata_df = pd.json_normalize(df['original_metadata'])
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    # çµ„åˆä¸€å€‹ä¹¾æ·¨ã€æ‰å¹³åŒ–çš„ DataFrame
    final_df = pd.DataFrame({
        'id': df['original_tweet_id'].astype(str),
        'cleaned_text': cleaned_data_df['cleaned_text'],
        'createdAt': pd.to_datetime(original_metadata_df['createdAt']),
        'original_data': df['original_metadata'] # ä¿ç•™å®Œæ•´çš„åŸå§‹è³‡æ–™ä»¥ä¾›é–±è®€çª—æ ¼ä½¿ç”¨
    })
    
    # ç§»é™¤ 'cleaned_text' ç‚ºç©ºå€¼çš„è¡Œï¼Œä»¥é¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(final_df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ (å·²æ¿¾é™¤ç©ºå…§å®¹èˆ‡é‡è¤‡é …)ã€‚")
    if len(final_df) < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - len(final_df)} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    return final_df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—å‡½æ•¸ (æ­¤å‡½æ•¸å·²ä¸å†éœ€è¦ï¼Œæ•…åˆªé™¤)
# ==============================================================================
# def clean_text(text):  <-- æ­¤å‡½æ•¸å·²è¢«ç§»é™¤

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        X = vectorizer.transform(group['cleaned_text'])
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡ RoBERTa ä¸‰åˆ†é¡æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT) - æ­¤éƒ¨åˆ†ä¸è®Š
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA) - æ­¤éƒ¨åˆ†ä¸è®Š
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC) - æ­¤éƒ¨åˆ†ä¸è®Š
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # D. æƒ…ç·’åˆ†æ (ä½¿ç”¨ RoBERTa æ¨¡å‹é€²è¡Œä¸‰åˆ†é¡)
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ (ä½¿ç”¨ Twitter RoBERTa æ¨¡å‹)...")
    
    # ============================  CHANGE starts here ============================
    # ä¿®æ­£ï¼šå°‡æ¨¡å‹æ›å› Twitter RoBERTa æ¨¡å‹
    sentiment_pipeline = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment-latest")
    # ============================= CHANGE ends here ==============================

    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    
    # é€™å€‹ sentiment_map ä¿æŒä¸è®Šï¼Œå› ç‚ºå®ƒå·²ç¶“åŒ…å«äº†è™•ç†ä¸‰ç¨®åˆ†é¡çš„é‚è¼¯
    sentiment_map = {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'}
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # æº–å‚™è¼¸å‡ºçµæœ - æ­¤éƒ¨åˆ†ä¸è®Š
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ (å·²æ›´æ–°)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘åœ¨è®€å–è³‡æ–™æ™‚ä¿ç•™çš„ 'original_data' æ¬„ä½
    # å°‡å…¶è½‰æ›ç‚ºä»¥ 'id' ç‚º key çš„å­—å…¸
    reading_pane_dict = pd.Series(df.original_data.values, index=df.id).to_dict()
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK (å·²æ›´æ–°)
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥ä¸¦è™•ç†æ–°æ ¼å¼çš„è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ–‡æœ¬æ¸…æ´—æ­¥é©Ÿå·²è¢«ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ç›´æ¥ä½¿ç”¨æª”æ¡ˆä¸­çš„ 'cleaned_text'
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")