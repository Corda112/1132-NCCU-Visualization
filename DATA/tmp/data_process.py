import pandas as pd
import numpy as np
import json
import glob
import re
import emoji
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡åŸºç¤è™•ç†
# ==============================================================================
def load_and_merge_data(directory="."):
    """
    è¼‰å…¥æŒ‡å®šè³‡æ–™å¤¾è£¡çš„æ‰€æœ‰ JSON æª”æ¡ˆä¸¦åˆä½µæˆä¸€å€‹ DataFrameã€‚
    """
    path_pattern = os.path.join(directory, "*.json")
    json_files = glob.glob(path_pattern)
    if not json_files:
        print(f"éŒ¯èª¤ï¼šåœ¨è³‡æ–™å¤¾ '{directory}' ä¸­æ‰¾ä¸åˆ°ä»»ä½• JSON æª”æ¡ˆã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    df = df.drop_duplicates(subset='id').reset_index(drop=True)
    
    # å°‡ 'createdAt' è½‰æ›ç‚º datetime ç‰©ä»¶ä»¥ä¾¿å¾ŒçºŒè™•ç†
    df['createdAt'] = pd.to_datetime(df['createdAt'])
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ã€‚")
    return df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—
# ==============================================================================
def clean_text(text):
    """
    æ ¹æ“š TextVista è«–æ–‡æè¿°ï¼Œå°å–®ç¯‡è²¼æ–‡é€²è¡Œæ·±åº¦æ¸…æ´—ã€‚
    """
    # 1. ç§»é™¤ç¶²å€
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # 2. ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿ
    text = emoji.demojize(text)
    text = re.sub(r':[a-zA-Z_]+:', ' ', text)
    # 3. ç§»é™¤ä½¿ç”¨è€…æ¨™è¨» (@mentions)
    text = re.sub(r'@\w+', '', text)
    # 4. ç§»é™¤ Hashtag çš„ '#' ç¬¦è™Ÿï¼Œä½†ä¿ç•™æ–‡å­—
    text = text.replace('#', '')
    # 5. ç§»é™¤è‚¡ç¥¨ä»£ç¢¼ '$' ç¬¦è™Ÿ
    text = text.replace('$', '')
    # 6. ç§»é™¤æ›è¡Œç¬¦èˆ‡å¤šé¤˜çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    # 7. çµ±ä¸€è½‰ç‚ºå°å¯«
    text = text.lower()
    
    return text

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    # ä½¿ç”¨ CountVectorizer é«˜æ•ˆç‡åœ°è¨ˆç®— Term å’Œ N-gram
    # ngram_range=(1, 3) è¡¨ç¤ºè¨ˆç®—å–®è©ã€2-gramã€3-gram
    # min_df=5 è¡¨ç¤ºä¸€å€‹è©æˆ–è©çµ„è‡³å°‘è¦åœ¨5ç¯‡æ–‡ä»¶ä¸­å‡ºç¾éæ‰è¨ˆç®—ï¼Œéæ¿¾é›œè¨Š
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    # æ“¬åˆæ•´å€‹èªæ–™åº«ä»¥ç¢ºå®šè©å½™è¡¨
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    # æŒ‰æ—¥æœŸåˆ†çµ„è¨ˆç®—é »ç‡
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        # åªè½‰æ›ç•¶å¤©çš„æ–‡æœ¬
        X = vectorizer.transform(group['cleaned_text'])
        # åŠ ç¸½å¾—åˆ°ç•¶å¤©æ‰€æœ‰è©å½™çš„é »ç‡
        freqs = X.sum(axis=0).A1
        # å»ºç«‹è©å½™åˆ°é »ç‡çš„æ˜ å°„
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT)
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA)
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC)
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # D. æƒ…ç·’åˆ†æ
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ...")
    sentiment_pipeline = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment-latest")
    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    sentiment_map = {'positive': 'Positive', 'neutral': 'Negative', 'negative': 'Negative'}
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # æº–å‚™è¼¸å‡ºçµæœ
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    
    # ============================  FIX starts here ============================
    # ä¿®æ­£ï¼šä½¿ç”¨ .apply() ä¾†è½‰æ›æ—¥æœŸæ ¼å¼
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    # ============================= FIX ends here ==============================
    
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    # Replace NaN with None for valid JSON output
    output_data.replace({np.nan: None}, inplace=True)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    df_indexed = df.set_index('id')
    
    # ============================  FIX starts here ============================
    # ä¿®æ­£ï¼šä½¿ç”¨ .apply() ä¾†è½‰æ›æ—¥æœŸæ ¼å¼
    df_indexed['createdAt'] = df_indexed['createdAt'].apply(lambda x: x.isoformat())
    # ============================= FIX ends here ==============================
    
    # Replace NaN with None for valid JSON output
    df_indexed.replace({np.nan: None}, inplace=True)
    
    reading_pane_dict = df_indexed.to_dict(orient='index')
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


# ==============================================================================
# MAIN EXECUTION BLOCK
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ¸…æ´—æ–‡æœ¬
        print("\n--- æ­£åœ¨é€²è¡Œæ–‡æœ¬æ¸…æ´— ---")
        main_df['cleaned_text'] = main_df['text'].apply(clean_text)
        print("æ–‡æœ¬æ¸…æ´—å®Œæˆã€‚")
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")