'''
# ä½¿ç”¨FinBertæ¨¡å‹åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡æ–°æ ¼å¼è™•ç† (å·²æ›´æ–°)
# ==============================================================================
def load_and_merge_data(path_pattern=None):
    """
    è¼‰å…¥æ‰€æœ‰ç¬¦åˆæ¨£å¼çš„ã€å·²æ¸…æ´—éçš„ JSON æª”æ¡ˆï¼Œä¸¦å¾å·¢ç‹€çµæ§‹ä¸­æå–æ‰€éœ€è³‡æ–™ã€‚
    å¦‚æœæœªæä¾› `path_pattern`ï¼Œæœƒè‡ªå‹•å¾ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„ '../processed' è³‡æ–™å¤¾å°‹æ‰¾ã€‚
    """
    if path_pattern is None:
        # æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª”æ¡ˆçµæ§‹ï¼Œè…³æœ¬åœ¨ 'DATA/tmp'ï¼Œè³‡æ–™åœ¨ 'DATA/processed'ã€‚
        # æˆ‘å€‘å°‡åŸºæ–¼è…³æœ¬çš„çµ•å°è·¯å¾‘ä¾†å»ºæ§‹è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"æœªæŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")

    json_files = glob.glob(path_pattern)
    if not json_files:
        # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•—ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
        searched_path = os.path.dirname(path_pattern)
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{searched_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„ 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'DATA/processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ä½¿ç”¨ pandas.json_normalize ä¾†é«˜æ•ˆåœ°è™•ç†å·¢ç‹€JSON
    # å¾ 'cleaned_data' å’Œ 'original_metadata' ä¸­æå–æˆ‘å€‘éœ€è¦çš„æ¬„ä½
    try:
        cleaned_data_df = pd.json_normalize(df['cleaned_data'])
        original_metadata_df = pd.json_normalize(df['original_metadata'])
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    # çµ„åˆä¸€å€‹ä¹¾æ·¨ã€æ‰å¹³åŒ–çš„ DataFrame
    final_df = pd.DataFrame({
        'id': df['original_tweet_id'].astype(str),
        'cleaned_text': cleaned_data_df['cleaned_text'],
        'createdAt': pd.to_datetime(original_metadata_df['createdAt']),
        'original_data': df['original_metadata'] # ä¿ç•™å®Œæ•´çš„åŸå§‹è³‡æ–™ä»¥ä¾›é–±è®€çª—æ ¼ä½¿ç”¨
    })
    
    # ç§»é™¤ 'cleaned_text' ç‚ºç©ºå€¼çš„è¡Œï¼Œä»¥é¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(final_df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ (å·²æ¿¾é™¤ç©ºå…§å®¹èˆ‡é‡è¤‡é …)ã€‚")
    if len(final_df) < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - len(final_df)} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    return final_df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—å‡½æ•¸ (æ­¤å‡½æ•¸å·²ä¸å†éœ€è¦ï¼Œæ•…åˆªé™¤)
# ==============================================================================
# def clean_text(text):  <-- æ­¤å‡½æ•¸å·²è¢«ç§»é™¤

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        X = vectorizer.transform(group['cleaned_text'])
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡ FinBERTa æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT)
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA)
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC)
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # D. æƒ…ç·’åˆ†æ (ä½¿ç”¨ FinBERTa)
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ (ä½¿ç”¨ FinBERTa æ¨¡å‹)...")
    sentiment_pipeline = pipeline("sentiment-analysis", model="ProsusAI/finbert")
    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    sentiment_map = {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'}
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # æº–å‚™è¼¸å‡ºçµæœ
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ (å·²æ›´æ–°)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘åœ¨è®€å–è³‡æ–™æ™‚ä¿ç•™çš„ 'original_data' æ¬„ä½
    # å°‡å…¶è½‰æ›ç‚ºä»¥ 'id' ç‚º key çš„å­—å…¸
    reading_pane_dict = pd.Series(df.original_data.values, index=df.id).to_dict()
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK (å·²æ›´æ–°)
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥ä¸¦è™•ç†æ–°æ ¼å¼çš„è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ–‡æœ¬æ¸…æ´—æ­¥é©Ÿå·²è¢«ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ç›´æ¥ä½¿ç”¨æª”æ¡ˆä¸­çš„ 'cleaned_text'
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")
'''







'''
# ä½¿ç”¨RoBertæ¨¡å‹åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡æ–°æ ¼å¼è™•ç† (å·²æ›´æ–°)
# ==============================================================================
def load_and_merge_data(path_pattern=None):
    """
    è¼‰å…¥æ‰€æœ‰ç¬¦åˆæ¨£å¼çš„ã€å·²æ¸…æ´—éçš„ JSON æª”æ¡ˆï¼Œä¸¦å¾å·¢ç‹€çµæ§‹ä¸­æå–æ‰€éœ€è³‡æ–™ã€‚
    å¦‚æœæœªæä¾› `path_pattern`ï¼Œæœƒè‡ªå‹•å¾ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„ '../processed' è³‡æ–™å¤¾å°‹æ‰¾ã€‚
    """
    if path_pattern is None:
        # æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª”æ¡ˆçµæ§‹ï¼Œè…³æœ¬åœ¨ 'DATA/tmp'ï¼Œè³‡æ–™åœ¨ 'DATA/processed'ã€‚
        # æˆ‘å€‘å°‡åŸºæ–¼è…³æœ¬çš„çµ•å°è·¯å¾‘ä¾†å»ºæ§‹è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"æœªæŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")

    json_files = glob.glob(path_pattern)
    if not json_files:
        # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•—ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
        searched_path = os.path.dirname(path_pattern)
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{searched_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„ 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'DATA/processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ä½¿ç”¨ pandas.json_normalize ä¾†é«˜æ•ˆåœ°è™•ç†å·¢ç‹€JSON
    # å¾ 'cleaned_data' å’Œ 'original_metadata' ä¸­æå–æˆ‘å€‘éœ€è¦çš„æ¬„ä½
    try:
        cleaned_data_df = pd.json_normalize(df['cleaned_data'])
        original_metadata_df = pd.json_normalize(df['original_metadata'])
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    # çµ„åˆä¸€å€‹ä¹¾æ·¨ã€æ‰å¹³åŒ–çš„ DataFrame
    final_df = pd.DataFrame({
        'id': df['original_tweet_id'].astype(str),
        'cleaned_text': cleaned_data_df['cleaned_text'],
        'createdAt': pd.to_datetime(original_metadata_df['createdAt']),
        'original_data': df['original_metadata'] # ä¿ç•™å®Œæ•´çš„åŸå§‹è³‡æ–™ä»¥ä¾›é–±è®€çª—æ ¼ä½¿ç”¨
    })
    
    # ç§»é™¤ 'cleaned_text' ç‚ºç©ºå€¼çš„è¡Œï¼Œä»¥é¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(final_df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ (å·²æ¿¾é™¤ç©ºå…§å®¹èˆ‡é‡è¤‡é …)ã€‚")
    if len(final_df) < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - len(final_df)} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    return final_df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—å‡½æ•¸ (æ­¤å‡½æ•¸å·²ä¸å†éœ€è¦ï¼Œæ•…åˆªé™¤)
# ==============================================================================
# def clean_text(text):  <-- æ­¤å‡½æ•¸å·²è¢«ç§»é™¤

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        X = vectorizer.transform(group['cleaned_text'])
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡ RoBERTa ä¸‰åˆ†é¡æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT) - æ­¤éƒ¨åˆ†ä¸è®Š
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA) - æ­¤éƒ¨åˆ†ä¸è®Š
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC) - æ­¤éƒ¨åˆ†ä¸è®Š
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # D. æƒ…ç·’åˆ†æ (ä½¿ç”¨ RoBERTa æ¨¡å‹é€²è¡Œä¸‰åˆ†é¡)
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ (ä½¿ç”¨ Twitter RoBERTa æ¨¡å‹)...")
    
    # ============================  CHANGE starts here ============================
    # ä¿®æ­£ï¼šå°‡æ¨¡å‹æ›å› Twitter RoBERTa æ¨¡å‹
    sentiment_pipeline = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment-latest")
    # ============================= CHANGE ends here ==============================

    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    
    # é€™å€‹ sentiment_map ä¿æŒä¸è®Šï¼Œå› ç‚ºå®ƒå·²ç¶“åŒ…å«äº†è™•ç†ä¸‰ç¨®åˆ†é¡çš„é‚è¼¯
    sentiment_map = {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'}
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # æº–å‚™è¼¸å‡ºçµæœ - æ­¤éƒ¨åˆ†ä¸è®Š
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ (å·²æ›´æ–°)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘åœ¨è®€å–è³‡æ–™æ™‚ä¿ç•™çš„ 'original_data' æ¬„ä½
    # å°‡å…¶è½‰æ›ç‚ºä»¥ 'id' ç‚º key çš„å­—å…¸
    reading_pane_dict = pd.Series(df.original_data.values, index=df.id).to_dict()
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK (å·²æ›´æ–°)
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥ä¸¦è™•ç†æ–°æ ¼å¼çš„è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ–‡æœ¬æ¸…æ´—æ­¥é©Ÿå·²è¢«ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ç›´æ¥ä½¿ç”¨æª”æ¡ˆä¸­çš„ 'cleaned_text'
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")
'''







'''
# ä½¿ç”¨CryptoBERTæ¨¡å‹åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å–èˆ‡æ–°æ ¼å¼è™•ç† (å·²æ›´æ–°)
# ==============================================================================
def load_and_merge_data(path_pattern=None):
    """
    è¼‰å…¥æ‰€æœ‰ç¬¦åˆæ¨£å¼çš„ã€å·²æ¸…æ´—éçš„ JSON æª”æ¡ˆï¼Œä¸¦å¾å·¢ç‹€çµæ§‹ä¸­æå–æ‰€éœ€è³‡æ–™ã€‚
    å¦‚æœæœªæä¾› `path_pattern`ï¼Œæœƒè‡ªå‹•å¾ç›¸å°æ–¼è…³æœ¬ä½ç½®çš„ '../processed' è³‡æ–™å¤¾å°‹æ‰¾ã€‚
    """
    if path_pattern is None:
        # æ ¹æ“šä½¿ç”¨è€…æä¾›çš„æª”æ¡ˆçµæ§‹ï¼Œè…³æœ¬åœ¨ 'DATA/tmp'ï¼Œè³‡æ–™åœ¨ 'DATA/processed'ã€‚
        # æˆ‘å€‘å°‡åŸºæ–¼è…³æœ¬çš„çµ•å°è·¯å¾‘ä¾†å»ºæ§‹è³‡æ–™æª”æ¡ˆçš„è·¯å¾‘ã€‚
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"æœªæŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")

    json_files = glob.glob(path_pattern)
    if not json_files:
        # å¦‚æœè‡ªå‹•åµæ¸¬å¤±æ•—ï¼Œæä¾›æ›´æ˜ç¢ºçš„éŒ¯èª¤è¨Šæ¯
        searched_path = os.path.dirname(path_pattern)
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{searched_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„ 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'DATA/processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    # ä½¿ç”¨ pandas.json_normalize ä¾†é«˜æ•ˆåœ°è™•ç†å·¢ç‹€JSON
    # å¾ 'cleaned_data' å’Œ 'original_metadata' ä¸­æå–æˆ‘å€‘éœ€è¦çš„æ¬„ä½
    try:
        cleaned_data_df = pd.json_normalize(df['cleaned_data'])
        original_metadata_df = pd.json_normalize(df['original_metadata'])
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    # çµ„åˆä¸€å€‹ä¹¾æ·¨ã€æ‰å¹³åŒ–çš„ DataFrame
    final_df = pd.DataFrame({
        'id': df['original_tweet_id'].astype(str),
        'cleaned_text': cleaned_data_df['cleaned_text'],
        'createdAt': pd.to_datetime(original_metadata_df['createdAt']),
        'original_data': df['original_metadata'] # ä¿ç•™å®Œæ•´çš„åŸå§‹è³‡æ–™ä»¥ä¾›é–±è®€çª—æ ¼ä½¿ç”¨
    })
    
    # ç§»é™¤ 'cleaned_text' ç‚ºç©ºå€¼çš„è¡Œï¼Œä»¥é¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    
    # ç§»é™¤é‡è¤‡çš„æ¨æ–‡ (ä»¥ id ç‚ºæº–)
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {len(final_df)} ç¯‡ä¸é‡è¤‡çš„è²¼æ–‡ (å·²æ¿¾é™¤ç©ºå…§å®¹èˆ‡é‡è¤‡é …)ã€‚")
    if len(final_df) < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - len(final_df)} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    return final_df

# ==============================================================================
# STEP 2: è³‡æ–™æ¸…æ´—å‡½æ•¸ (æ­¤å‡½æ•¸å·²ä¸å†éœ€è¦ï¼Œæ•…åˆªé™¤)
# ==============================================================================
# def clean_text(text):  <-- æ­¤å‡½æ•¸å·²è¢«ç§»é™¤

# ==============================================================================
# STEP 3: ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=5)
    
    vectorizer.fit(df['cleaned_text'])
    
    daily_freq = {}
    
    for date, group in df.groupby('date'):
        print(f"æ­£åœ¨è™•ç† {date} çš„æ•¸æ“š...")
        X = vectorizer.transform(group['cleaned_text'])
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        daily_freq[date] = term_freqs
        
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€åˆ†ç¾¤èˆ‡ CryptoBERT ä¸‰åˆ†é¡æƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    
    df_sample = df.copy()
    print(f"å°‡å° {len(df_sample)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    # --- ä»¥ä¸‹æ­¥é©Ÿç¶­æŒä¸è®Š ---
    # A. æ–‡æœ¬å‘é‡åŒ– (SBERT)
    print("æ­¥é©Ÿ 1/4: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡ (æ­¤æ­¥é©Ÿå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“)...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df_sample['cleaned_text'].tolist(), show_progress_bar=True)
    
    # B. é™ç¶­ (PCA)
    print("æ­¥é©Ÿ 2/4: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df_sample['x'] = coordinates[:, 0]
    df_sample['y'] = coordinates[:, 1]
    
    # C. åˆ†ç¾¤ (HAC)
    n_clusters = 20
    print(f"æ­¥é©Ÿ 3/4: æ­£åœ¨ä½¿ç”¨ HAC é€²è¡Œåˆ†ç¾¤ (åˆ†æˆ {n_clusters} ç¾¤)...")
    hac = AgglomerativeClustering(n_clusters=n_clusters)
    df_sample['cluster_id'] = hac.fit_predict(embeddings)
    
    # --- æƒ…ç·’åˆ†ææ­¥é©Ÿæ›´æ–° ---
    # D. æƒ…ç·’åˆ†æ (ä½¿ç”¨ CryptoBERT æ¨¡å‹)
    print("æ­¥é©Ÿ 4/4: æ­£åœ¨é€²è¡Œæƒ…ç·’åˆ†æ (ä½¿ç”¨ ElKulako/cryptobert æ¨¡å‹)...")
    
    # ============================  CHANGE 1 starts here ============================
    # ä¿®æ­£ï¼šå°‡æ¨¡å‹æ›´æ›ç‚ºæ‚¨æŒ‡å®šçš„ CryptoBERT æ¨¡å‹
    sentiment_pipeline = pipeline("text-classification", model="ElKulako/cryptobert")
    # ============================= CHANGE 1 ends here ==============================

    sentiments = sentiment_pipeline(
        df_sample['cleaned_text'].tolist(), 
        batch_size=64, 
        truncation=True, 
        max_length=512
    )
    
    # ============================  CHANGE 2 starts here ============================
    # ä¿®æ­£ï¼šæ›´æ–°æƒ…ç·’æ¨™ç±¤å°æ‡‰ï¼Œä»¥ç¬¦åˆ CryptoBERT çš„è¼¸å‡º
    # LABEL_2 -> Positive, LABEL_1 -> Neutral, LABEL_0 -> Negative
    sentiment_map = {'Bullish': 'Positive', 'Bearish': 'Negative', 'Neutral': 'Neutral'}
    # ============================= CHANGE 2 ends here ==============================
    
    df_sample['sentiment'] = [sentiment_map[s['label']] for s in sentiments]

    # --- æº–å‚™è¼¸å‡ºçµæœï¼Œæ­¤éƒ¨åˆ†ä¸è®Š ---
    output_data = df_sample[[
        'id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y'
    ]].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    output_data['x'] = output_data['x'].astype(float)
    output_data['y'] = output_data['y'].astype(float)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
        
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="reading_pane_data.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°åŸå§‹è²¼æ–‡è³‡æ–™çš„æ˜ å°„ (å·²æ›´æ–°)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘åœ¨è®€å–è³‡æ–™æ™‚ä¿ç•™çš„ 'original_data' æ¬„ä½
    # å°‡å…¶è½‰æ›ç‚ºä»¥ 'id' ç‚º key çš„å­—å…¸
    reading_pane_dict = pd.Series(df.original_data.values, index=df.id).to_dict()
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK (å·²æ›´æ–°)
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    
    # 1. è¼‰å…¥ä¸¦è™•ç†æ–°æ ¼å¼çš„è³‡æ–™
    main_df = load_and_merge_data()
    
    if main_df is not None:
        # 2. æ–‡æœ¬æ¸…æ´—æ­¥é©Ÿå·²è¢«ç§»é™¤ï¼Œå› ç‚ºæˆ‘å€‘ç›´æ¥ä½¿ç”¨æª”æ¡ˆä¸­çš„ 'cleaned_text'
        
        # 3. ç”¢ç”Ÿä¸‰å€‹è¼¸å‡ºæª”æ¡ˆ
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {end_time - start_time:.2f} ç§’ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ reading_pane_data.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")
        '''




# ä½¿ç”¨é›†æˆæ¨¡å‹ä¾æ¬Šé‡æŠ•ç¥¨åˆ†é¡æƒ…ç·’ï¼Œå…±åˆ†æˆæ­£ã€åã€ä¸€èˆ¬ä¸‰é¡
import pandas as pd
import numpy as np
import json
import glob
import time
import os

# --- NLP & ML Libraries ---
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score  # å°å…¥è¼ªå»“ä¿‚æ•¸è¨ˆç®—å·¥å…·
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- Utility to handle potential warnings ---
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# STEP 1: æ•¸æ“šè®€å– (å·²ä¿®æ­£è·¯å¾‘å•é¡Œ)
# ==============================================================================
def load_and_merge_data():
    """
    è‡ªå‹•å¾è…³æœ¬çš„ç›¸å°ä½ç½® '../processed' è³‡æ–™å¤¾è¼‰å…¥æ‰€æœ‰ 'cleaned_*.json' æª”æ¡ˆã€‚
    """
    try:
        # ç²å–ç•¶å‰è…³æœ¬çš„çµ•å°è·¯å¾‘
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # æ§‹å»ºç›®æ¨™è³‡æ–™å¤¾çš„è·¯å¾‘ (å¾ tmp ä¸Šä¸€å±¤åˆ° processed)
        data_dir = os.path.abspath(os.path.join(script_dir, '..', 'processed'))
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')
        print(f"è‡ªå‹•åœ¨è³‡æ–™å¤¾ '{data_dir}' ä¸­æœå°‹ 'cleaned_*.json' æª”æ¡ˆ...")
    except NameError:
        # å¦‚æœåœ¨äº’å‹•å¼ç’°å¢ƒ (å¦‚ Jupyter) ä¸­é‹è¡Œï¼Œ__file__ å¯èƒ½ä¸å­˜åœ¨
        print("åœ¨äº’å‹•å¼ç’°å¢ƒä¸­é‹è¡Œï¼Œè«‹ç¢ºä¿æ‚¨çš„å·¥ä½œç›®éŒ„æ­£ç¢ºã€‚")
        data_dir = os.path.join('..', 'processed')
        path_pattern = os.path.join(data_dir, 'cleaned_*.json')

    json_files = glob.glob(path_pattern)
    if not json_files:
        print(f"éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{os.path.dirname(path_pattern)}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'cleaned_*.json' çš„æª”æ¡ˆã€‚")
        print("è«‹ç¢ºèªæ‚¨çš„è…³æœ¬ä½æ–¼ 'tmp' è³‡æ–™å¤¾ï¼Œä¸” 'cleaned_*.json' æª”æ¡ˆç¢ºå¯¦å­˜æ”¾åœ¨ 'processed' è³‡æ–™å¤¾ä¸­ã€‚")
        return None
        
    print(f"æ‰¾åˆ° {len(json_files)} å€‹å·²æ¸…æ´—çš„æª”æ¡ˆ: {json_files}")
    
    df_list = [pd.read_json(f) for f in json_files]
    df = pd.concat(df_list, ignore_index=True)
    
    try:
        final_df = pd.DataFrame({
            'id': df['original_tweet_id'].astype(str),
            'cleaned_text': pd.json_normalize(df['cleaned_data'])['cleaned_text'],
            'createdAt': pd.to_datetime(pd.json_normalize(df['original_metadata'])['createdAt']),
            'processing_timestamp': df['processing_timestamp']
        })
    except KeyError as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å…¥çš„ JSON æª”æ¡ˆç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼š{e}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆçµæ§‹ã€‚")
        return None

    initial_count = len(final_df)
    final_df.dropna(subset=['cleaned_text'], inplace=True)
    final_df = final_df[final_df['cleaned_text'] != '']
    final_df = final_df.drop_duplicates(subset='id').reset_index(drop=True)
    
    final_count = len(final_df)
    if final_count < initial_count:
        print(f"æ³¨æ„ï¼šåœ¨æ¸…ç†éç¨‹ä¸­ç§»é™¤äº† {initial_count - final_count} ç­†è³‡æ–™ (å› å…§å®¹ç‚ºç©ºæˆ–é‡è¤‡)ã€‚")
        
    print(f"æˆåŠŸè¼‰å…¥ä¸¦åˆä½µè³‡æ–™ï¼Œå…± {final_count} ç¯‡æœ‰æ•ˆè²¼æ–‡ã€‚")
    return final_df

# ==============================================================================
# STEP 2: æª”æ¡ˆç”Ÿæˆ
# ==============================================================================

def generate_frequency_file(df, output_filename="term_ngram_frequency.json"):
    """
    æª”æ¡ˆ1: è¨ˆç®—æ¯æ—¥çš„ Term å’Œ N-gram é »ç‡ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿè©å½™é »ç‡æª”æ¡ˆ (æª”æ¡ˆ 1/3) ---")
    df['date'] = df['createdAt'].dt.date.astype(str)
    vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english', min_df=10)
    
    # ç¢ºä¿ cleaned_text æ˜¯å­—ä¸²é¡å‹
    vectorizer.fit(df['cleaned_text'].astype(str))
    
    daily_freq = {}
    for date, group in df.groupby('date'):
        if group.empty: continue
        X = vectorizer.transform(group['cleaned_text'].astype(str))
        freqs = X.sum(axis=0).A1
        term_freqs = {term: int(freq) for term, freq in zip(vectorizer.get_feature_names_out(), freqs) if freq > 0}
        if term_freqs:
            daily_freq[date] = term_freqs
            
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(daily_freq, f, ensure_ascii=False, indent=4)
    print(f"âœ… è©å½™é »ç‡æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_semantic_file(df, output_filename="semantic_clustering_sentiment.json"):
    """
    æª”æ¡ˆ2: é€²è¡Œå‘é‡åŒ–ã€é™ç¶­ã€è‡ªé©æ‡‰åˆ†ç¾¤èˆ‡æ¨¡å‹é›†æˆæƒ…ç·’åˆ†æã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿèªæ„åˆ†ææª”æ¡ˆ (æª”æ¡ˆ 2/3) ---")
    print(f"å°‡å° {len(df)} ç¯‡æ–‡ç« é€²è¡Œèªæ„åˆ†æ...")

    print("â¡ï¸ æ­¥é©Ÿ 1/5: æ­£åœ¨ä½¿ç”¨ SBERT ç”¢ç”Ÿæ–‡æœ¬å‘é‡...")
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = sbert_model.encode(df['cleaned_text'].astype(str).tolist(), show_progress_bar=True)
    
    print("â¡ï¸ æ­¥é©Ÿ 2/5: æ­£åœ¨ä½¿ç”¨ PCA é€²è¡Œé™ç¶­...")
    pca = PCA(n_components=2, random_state=42)
    coordinates = pca.fit_transform(embeddings)
    df['x'] = coordinates[:, 0]
    df['y'] = coordinates[:, 1]
    
    def find_optimal_clusters(embeddings, max_clusters=30):
        print("â¡ï¸ æ­¥é©Ÿ 3/5: æ­£åœ¨ä½¿ç”¨è¼ªå»“ä¿‚æ•¸å°‹æ‰¾æœ€ä½³åˆ†ç¾¤æ•¸é‡ (æ­¤æ­¥é©Ÿè¼ƒè€—æ™‚)...")
        sample_size = 5000
        if len(embeddings) > sample_size:
            indices = np.random.choice(len(embeddings), sample_size, replace=False)
            embeddings_sample = embeddings[indices]
        else:
            embeddings_sample = embeddings

        best_score = -1
        best_n_clusters = -1
        cluster_range = range(5, max_clusters + 1, 5)
        
        for n_clusters in cluster_range:
            clusterer = AgglomerativeClustering(n_clusters=n_clusters)
            cluster_labels = clusterer.fit_predict(embeddings_sample)
            score = silhouette_score(embeddings_sample, cluster_labels)
            print(f"  æ¸¬è©¦åˆ†ç¾¤æ•¸ = {n_clusters}, è¼ªå»“ä¿‚æ•¸ = {score:.4f}")
            if score > best_score:
                best_score = score
                best_n_clusters = n_clusters
                
        print(f"âœ¨ æ‰¾åˆ°æœ€ä½³åˆ†ç¾¤æ•¸: {best_n_clusters} (è¼ªå»“ä¿‚æ•¸æœ€é«˜: {best_score:.4f})")
        return best_n_clusters

    optimal_n_clusters = find_optimal_clusters(embeddings)
    
    print(f"â¡ï¸ æ­¥é©Ÿ 4/5: æ­£åœ¨ä½¿ç”¨ {optimal_n_clusters} ç¾¤é€²è¡Œæœ€çµ‚åˆ†ç¾¤...")
    final_clusterer = AgglomerativeClustering(n_clusters=optimal_n_clusters)
    df['cluster_id'] = final_clusterer.fit_predict(embeddings)
    
    print("â¡ï¸ æ­¥é©Ÿ 5/5: æ­£åœ¨é€²è¡Œæ¨¡å‹é›†æˆæƒ…ç·’åˆ†æ...")
    print("   æ³¨æ„ï¼šæ­¤æ­¥é©Ÿå°‡è¼‰å…¥ä¸‰å€‹æ¨¡å‹ï¼Œéœ€è¦è¼ƒå¤šè¨˜æ†¶é«”èˆ‡æ™‚é–“ã€‚")
    models = {
        'CryptoBERT': pipeline("text-classification", model="ElKulako/cryptobert"),
        'RoBERTa': pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest"),
        'FinBERT': pipeline("text-classification", model="ProsusAI/finbert")
    }
    weights = {
        'CryptoBERT': {'Positive': 0.50, 'Neutral': 0.30, 'Negative': 0.20},
        'RoBERTa':    {'Positive': 0.30, 'Neutral': 0.30, 'Negative': 0.40},
        'FinBERT':    {'Positive': 0.20, 'Neutral': 0.40, 'Negative': 0.30}
    }
    label_maps = {
        'CryptoBERT': {'LABEL_2': 'Positive', 'LABEL_1': 'Neutral', 'LABEL_0': 'Negative'},
        'RoBERTa':    {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'},
        'FinBERT':    {'positive': 'Positive', 'neutral': 'Neutral', 'negative': 'Negative'}
    }

    texts_to_analyze = df['cleaned_text'].astype(str).tolist()
    all_results = {}
    for name, pipe in models.items():
        print(f"   æ­£åœ¨ä½¿ç”¨ {name} é€²è¡Œé æ¸¬...")
        all_results[name] = pipe(texts_to_analyze, top_k=None, batch_size=32, truncation=True, max_length=512)

    print("   æ­£åœ¨è¨ˆç®—åŠ æ¬Šç¸½åˆ†...")
    final_sentiments = []
    for i in range(len(texts_to_analyze)):
        final_scores = {'Positive': 0.0, 'Neutral': 0.0, 'Negative': 0.0}
        for model_name, predictions in all_results.items():
            for pred in predictions[i]:
                raw_label = pred['label']
                if raw_label in label_maps[model_name]:
                    standard_label = label_maps[model_name][raw_label]
                    weight = weights[model_name][standard_label]
                    final_scores[standard_label] += pred['score'] * weight
        final_sentiments.append(max(final_scores, key=final_scores.get))
        
    df['sentiment'] = final_sentiments
    
    output_data = df[['id', 'cleaned_text', 'createdAt', 'cluster_id', 'sentiment', 'x', 'y']].copy()
    output_data['createdAt'] = output_data['createdAt'].apply(lambda x: x.isoformat())
    output_data['cluster_id'] = output_data['cluster_id'].astype(int)
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data.to_dict('records'), f, ensure_ascii=False, indent=4)
    print(f"âœ… èªæ„åˆ†ææª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")


def generate_reading_pane_file(df, output_filename="read_pane.json"):
    """
    æª”æ¡ˆ3: å»ºç«‹ä¸€å€‹å¾ ID åˆ°æŒ‡å®šæ¬„ä½çš„æ˜ å°„ (å·²ä¿®æ­£)ã€‚
    """
    print("\n--- é–‹å§‹ç”¢ç”Ÿé–±è®€çª—æ ¼å°ç…§æª” (æª”æ¡ˆ 3/3) ---")
    
    # æ ¹æ“šéœ€æ±‚ï¼Œåªé¸æ“‡ original_tweet_id (å³ 'id'), cleaned_text, processing_timestamp
    read_pane_df = df[['id', 'cleaned_text', 'processing_timestamp']].copy()
    read_pane_df.rename(columns={'id': 'original_tweet_id'}, inplace=True)
    
    read_pane_df.set_index('original_tweet_id', inplace=True)
    reading_pane_dict = read_pane_df.to_dict(orient='index')
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(reading_pane_dict, f, ensure_ascii=False, indent=4)
    print(f"âœ… é–±è®€çª—æ ¼æª”æ¡ˆ '{output_filename}' å·²æˆåŠŸå»ºç«‹ã€‚")

# ==============================================================================
# MAIN EXECUTION BLOCK
# ==============================================================================
if __name__ == "__main__":
    start_time = time.time()
    main_df = load_and_merge_data()
    
    if main_df is not None and not main_df.empty:
        generate_frequency_file(main_df)
        generate_semantic_file(main_df)
        generate_reading_pane_file(main_df)
        
        end_time = time.time()
        print(f"\nğŸ‰ æ‰€æœ‰è™•ç†å®Œæˆï¼ç¸½å…±èŠ±è²»: {(end_time - start_time) / 60:.2f} åˆ†é˜ã€‚")
        print("æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨ term_ngram_frequency.json, semantic_clustering_sentiment.json å’Œ read_pane.json é€²è¡Œè¦–è¦ºåŒ–ã€‚")
    else:
        print("\nè³‡æ–™è¼‰å…¥å¤±æ•—æˆ–ç„¡æœ‰æ•ˆè³‡æ–™ï¼Œç¨‹å¼å·²çµ‚æ­¢ã€‚")