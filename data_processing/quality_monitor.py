"""
è³‡æ–™å“è³ªç›£æ§æ¨¡çµ„
ç›£æ§è³‡æ–™è™•ç†éç¨‹ä¸­çš„å“è³ªæŒ‡æ¨™ï¼Œä¸¦ç”Ÿæˆå“è³ªå ±å‘Š
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from collections import Counter, defaultdict
import json
from pathlib import Path


class QualityMonitor:
    """è³‡æ–™å“è³ªç›£æ§å™¨ï¼šè¿½è¹¤å’Œåˆ†æè³‡æ–™è™•ç†å“è³ª"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å“è³ªç›£æ§å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.quality_config = config["processing"]["quality_checks"]
        self.logger = logging.getLogger("data_processor.quality_monitor")
        
        # åˆå§‹åŒ–çµ±è¨ˆæ•¸æ“š
        self.reset_stats()
        
        self.logger.info("QualityMonitor åˆå§‹åŒ–å®Œæˆ")
    
    def reset_stats(self):
        """é‡ç½®çµ±è¨ˆæ•¸æ“š"""
        self.stats = {
            "total_processed": 0,
            "successful_cleaned": 0,
            "retweets_filtered": 0,
            "empty_after_cleaning": 0,
            "processing_errors": 0,
            "text_length_distribution": defaultdict(int),
            "token_count_distribution": defaultdict(int),
            "processing_steps_stats": {
                "urls_mentions_removed": 0,
                "emojis_converted": 0,
                "stopwords_removed": 0,
                "lemmatized": 0
            },
            "quality_issues": {
                "too_short": 0,
                "too_long": 0,
                "no_meaningful_content": 0
            },
            "special_tokens": {
                "crypto_symbols": Counter(),
                "emoji_types": Counter(),
                "hashtags": Counter()
            },
            "processing_times": []
        }
    
    def monitor_batch(self, 
                     raw_texts: List[str], 
                     cleaned_results: List[Optional[Dict[str, Any]]],
                     processing_time: float) -> Dict[str, Any]:
        """
        ç›£æ§ä¸€å€‹æ‰¹æ¬¡çš„è™•ç†çµæœ
        
        Args:
            raw_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            cleaned_results: æ¸…æ´—çµæœåˆ—è¡¨
            processing_time: è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            æ‰¹æ¬¡å“è³ªå ±å‘Š
        """
        batch_stats = {
            "batch_size": len(raw_texts),
            "successful_rate": 0,
            "retweet_rate": 0,
            "processing_time": processing_time,
            "avg_processing_time_per_item": processing_time / len(raw_texts) if raw_texts else 0
        }
        
        successful_count = 0
        retweet_count = 0
        
        for i, (raw_text, result) in enumerate(zip(raw_texts, cleaned_results)):
            self.stats["total_processed"] += 1
            
            # åˆ†æåŸå§‹æ–‡æœ¬
            if raw_text and raw_text.strip().startswith("RT "):
                retweet_count += 1
                self.stats["retweets_filtered"] += 1
            
            # åˆ†æè™•ç†çµæœ
            if result is None:
                if raw_text and not raw_text.strip().startswith("RT "):
                    self.stats["processing_errors"] += 1
            else:
                successful_count += 1
                self.stats["successful_cleaned"] += 1
                self._analyze_cleaned_result(result)
        
        # è¨ˆç®—æ‰¹æ¬¡çµ±è¨ˆ
        batch_stats["successful_rate"] = successful_count / len(raw_texts) if raw_texts else 0
        batch_stats["retweet_rate"] = retweet_count / len(raw_texts) if raw_texts else 0
        
        # è¨˜éŒ„è™•ç†æ™‚é–“
        self.stats["processing_times"].append(processing_time)
        
        # å“è³ªæª¢æ ¸
        quality_alerts = self._check_quality_thresholds(batch_stats)
        
        return {
            "batch_stats": batch_stats,
            "quality_alerts": quality_alerts
        }
    
    def _analyze_cleaned_result(self, result: Dict[str, Any]):
        """åˆ†æå–®å€‹æ¸…æ´—çµæœ"""
        # æ–‡æœ¬é•·åº¦åˆ†æ
        original_length = len(result["original_text"])
        cleaned_length = len(result["cleaned_text"])
        token_count = result["token_count"]
        
        # é•·åº¦åˆ†ä½ˆçµ±è¨ˆ
        length_category = self._categorize_length(original_length)
        self.stats["text_length_distribution"][length_category] += 1
        
        token_category = self._categorize_token_count(token_count)
        self.stats["token_count_distribution"][token_category] += 1
        
        # è™•ç†æ­¥é©Ÿçµ±è¨ˆ
        processing_steps = result["processing_steps"]
        for step, occurred in processing_steps.items():
            if occurred:
                self.stats["processing_steps_stats"][step] += 1
        
        # å“è³ªå•é¡Œæª¢æ¸¬
        if original_length < self.quality_config["min_text_length"]:
            self.stats["quality_issues"]["too_short"] += 1
        elif original_length > self.quality_config["max_text_length"]:
            self.stats["quality_issues"]["too_long"] += 1
        
        if token_count == 0:
            self.stats["quality_issues"]["no_meaningful_content"] += 1
            self.stats["empty_after_cleaning"] += 1
        
        # ç‰¹æ®Štokenåˆ†æ
        self._analyze_special_tokens(result["final_tokens"])
    
    def _categorize_length(self, length: int) -> str:
        """å°‡æ–‡æœ¬é•·åº¦åˆ†é¡"""
        if length < 50:
            return "very_short"
        elif length < 100:
            return "short"
        elif length < 200:
            return "medium"
        elif length < 300:
            return "long"
        else:
            return "very_long"
    
    def _categorize_token_count(self, count: int) -> str:
        """å°‡tokenæ•¸é‡åˆ†é¡"""
        if count == 0:
            return "empty"
        elif count < 5:
            return "very_few"
        elif count < 10:
            return "few"
        elif count < 20:
            return "medium"
        elif count < 50:
            return "many"
        else:
            return "very_many"
    
    def _analyze_special_tokens(self, tokens: List[str]):
        """åˆ†æç‰¹æ®Štoken"""
        import re
        
        crypto_pattern = re.compile(r'\$[a-zA-Z]{2,10}')
        emoji_pattern = re.compile(r':[a-zA-Z_]+:')
        
        for token in tokens:
            # åŠ å¯†è²¨å¹£ç¬¦è™Ÿ
            if crypto_pattern.match(token):
                self.stats["special_tokens"]["crypto_symbols"][token] += 1
            
            # Emojiæè¿°
            elif emoji_pattern.match(token):
                self.stats["special_tokens"]["emoji_types"][token] += 1
            
            # Hashtagï¼ˆå·²ç§»é™¤#çš„ï¼‰
            elif token.lower() in ["bitcoin", "crypto", "defi", "eth", "btc"]:
                self.stats["special_tokens"]["hashtags"][token] += 1
    
    def _check_quality_thresholds(self, batch_stats: Dict[str, Any]) -> List[str]:
        """æª¢æŸ¥å“è³ªé–¾å€¼ï¼Œç”Ÿæˆè­¦å‘Š"""
        alerts = []
        
        # è½‰æ¨æ¯”ä¾‹è­¦å‘Š
        if batch_stats["retweet_rate"] > self.quality_config["retweet_threshold"]:
            alerts.append(
                f"é«˜è½‰æ¨æ¯”ä¾‹è­¦å‘Š: {batch_stats['retweet_rate']:.2%} "
                f"(é–¾å€¼: {self.quality_config['retweet_threshold']:.2%})"
            )
        
        # æˆåŠŸç‡è­¦å‘Š
        if batch_stats["successful_rate"] < 0.8:
            alerts.append(
                f"ä½æˆåŠŸç‡è­¦å‘Š: {batch_stats['successful_rate']:.2%} "
                f"(å»ºè­° > 80%)"
            )
        
        # è™•ç†é€Ÿåº¦è­¦å‘Š
        if batch_stats["avg_processing_time_per_item"] > 0.1:
            alerts.append(
                f"è™•ç†é€Ÿåº¦è­¦å‘Š: {batch_stats['avg_processing_time_per_item']:.3f}ç§’/æ¢ "
                f"(å»ºè­° < 0.1ç§’/æ¢)"
            )
        
        return alerts
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„å“è³ªå ±å‘Š"""
        if self.stats["total_processed"] == 0:
            return {"error": "å°šæœªè™•ç†ä»»ä½•è³‡æ–™"}
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        overall_stats = {
            "total_processed": self.stats["total_processed"],
            "successful_rate": self.stats["successful_cleaned"] / self.stats["total_processed"],
            "retweet_rate": self.stats["retweets_filtered"] / self.stats["total_processed"],
            "error_rate": self.stats["processing_errors"] / self.stats["total_processed"],
            "empty_after_cleaning_rate": self.stats["empty_after_cleaning"] / self.stats["total_processed"]
        }
        
        # è™•ç†æ•ˆèƒ½çµ±è¨ˆ
        performance_stats = {}
        if self.stats["processing_times"]:
            import statistics
            times = self.stats["processing_times"]
            performance_stats = {
                "total_batches": len(times),
                "total_processing_time": sum(times),
                "avg_batch_time": statistics.mean(times),
                "median_batch_time": statistics.median(times),
                "max_batch_time": max(times),
                "min_batch_time": min(times)
            }
        
        # å“è³ªåˆ†æ
        quality_analysis = {
            "text_length_distribution": dict(self.stats["text_length_distribution"]),
            "token_count_distribution": dict(self.stats["token_count_distribution"]),
            "processing_steps_effectiveness": self._calculate_step_effectiveness(),
            "quality_issues_summary": dict(self.stats["quality_issues"]),
            "special_tokens_summary": {
                "top_crypto_symbols": dict(self.stats["special_tokens"]["crypto_symbols"].most_common(10)),
                "top_emojis": dict(self.stats["special_tokens"]["emoji_types"].most_common(10)),
                "top_hashtags": dict(self.stats["special_tokens"]["hashtags"].most_common(10))
            }
        }
        
        # å»ºè­°å’Œè­¦å‘Š
        recommendations = self._generate_recommendations(overall_stats, quality_analysis)
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "overall_statistics": overall_stats,
            "performance_statistics": performance_stats,
            "quality_analysis": quality_analysis,
            "recommendations": recommendations,
            "config_used": self.quality_config
        }
        
        return report
    
    def _calculate_step_effectiveness(self) -> Dict[str, float]:
        """è¨ˆç®—å„è™•ç†æ­¥é©Ÿçš„æ•ˆæœ"""
        total_successful = self.stats["successful_cleaned"]
        if total_successful == 0:
            return {}
        
        effectiveness = {}
        for step, count in self.stats["processing_steps_stats"].items():
            effectiveness[step] = count / total_successful
        
        return effectiveness
    
    def _generate_recommendations(self, 
                                overall_stats: Dict[str, Any], 
                                quality_analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # æˆåŠŸç‡å»ºè­°
        if overall_stats["successful_rate"] < 0.9:
            recommendations.append(
                f"æˆåŠŸç‡è¼ƒä½ ({overall_stats['successful_rate']:.2%})ï¼Œ"
                "å»ºè­°æª¢æŸ¥æ–‡æœ¬å“è³ªæˆ–èª¿æ•´æ¸…æ´—åƒæ•¸"
            )
        
        # è½‰æ¨æ¯”ä¾‹å»ºè­°
        if overall_stats["retweet_rate"] > 0.15:
            recommendations.append(
                f"è½‰æ¨æ¯”ä¾‹è¼ƒé«˜ ({overall_stats['retweet_rate']:.2%})ï¼Œ"
                "å¯èƒ½éœ€è¦èª¿æ•´è³‡æ–™æ”¶é›†ç­–ç•¥"
            )
        
        # ç©ºå…§å®¹å»ºè­°
        if overall_stats["empty_after_cleaning_rate"] > 0.1:
            recommendations.append(
                f"æ¸…æ´—å¾Œç©ºå…§å®¹æ¯”ä¾‹è¼ƒé«˜ ({overall_stats['empty_after_cleaning_rate']:.2%})ï¼Œ"
                "å»ºè­°èª¿æ•´åœç”¨è©è¨­å®šæˆ–æ–‡æœ¬è™•ç†åƒæ•¸"
            )
        
        # Tokenåˆ†ä½ˆå»ºè­°
        token_dist = quality_analysis["token_count_distribution"]
        total_successful = overall_stats["successful_rate"] * overall_stats.get("total_processed", 1)
        if token_dist.get("empty", 0) + token_dist.get("very_few", 0) > total_successful * 0.2:
            recommendations.append(
                "éå¤šçŸ­æ–‡æœ¬ï¼Œå»ºè­°æª¢æŸ¥è³‡æ–™ä¾†æºå“è³ªæˆ–èª¿æ•´æœ€å°æ–‡æœ¬é•·åº¦é–¾å€¼"
            )
        
        # ç‰¹æ®Štokenå»ºè­°
        crypto_symbols = quality_analysis["special_tokens_summary"]["top_crypto_symbols"]
        if crypto_symbols:
            recommendations.append(
                f"ç™¼ç¾ {len(crypto_symbols)} ç¨®åŠ å¯†è²¨å¹£ç¬¦è™Ÿï¼Œ"
                "æ–‡æœ¬æ¸…æ´—å™¨æˆåŠŸä¿ç•™äº†ç‰¹æ®Šç¬¦è™Ÿ"
            )
        
        if not recommendations:
            recommendations.append("è³‡æ–™å“è³ªè‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥èª¿æ•´")
        
        return recommendations
    
    def save_report(self, output_path: Path) -> bool:
        """å„²å­˜å“è³ªå ±å‘Šåˆ°æª”æ¡ˆ"""
        try:
            report = self.generate_report()
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"å“è³ªå ±å‘Šå·²å„²å­˜è‡³: {output_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"å ±å‘Šå„²å­˜å¤±æ•—: {e}")
            return False
    
    def print_summary(self):
        """æ‰“å°ç°¡è¦çµ±è¨ˆæ‘˜è¦"""
        if self.stats["total_processed"] == 0:
            print("å°šæœªè™•ç†ä»»ä½•è³‡æ–™")
            return
        
        total = self.stats["total_processed"]
        successful = self.stats["successful_cleaned"]
        retweets = self.stats["retweets_filtered"]
        errors = self.stats["processing_errors"]
        
        print("\n" + "="*50)
        print("ğŸ“Š è³‡æ–™è™•ç†å“è³ªæ‘˜è¦")
        print("="*50)
        print(f"ç¸½è™•ç†æ•¸é‡: {total:,}")
        print(f"æˆåŠŸæ¸…æ´—: {successful:,} ({successful/total:.2%})")
        print(f"è½‰æ¨éæ¿¾: {retweets:,} ({retweets/total:.2%})")
        print(f"è™•ç†éŒ¯èª¤: {errors:,} ({errors/total:.2%})")
        
        if self.stats["processing_times"]:
            avg_time = sum(self.stats["processing_times"]) / len(self.stats["processing_times"])
            print(f"å¹³å‡æ‰¹æ¬¡è™•ç†æ™‚é–“: {avg_time:.2f}ç§’")
        
        # é¡¯ç¤ºå‰5å€‹åŠ å¯†è²¨å¹£ç¬¦è™Ÿ
        top_crypto = self.stats["special_tokens"]["crypto_symbols"].most_common(5)
        if top_crypto:
            print(f"\nğŸª™ ç†±é–€åŠ å¯†è²¨å¹£ç¬¦è™Ÿ:")
            for symbol, count in top_crypto:
                print(f"  {symbol}: {count}")
        
        print("="*50) 