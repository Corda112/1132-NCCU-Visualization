"""
æ–‡æœ¬æ¸…æ´—æ ¸å¿ƒæ¨¡çµ„
å¯¦ä½œå…­å€‹ä¸»è¦é è™•ç†æ­¥é©Ÿï¼Œé‡å°Twitteræ¨æ–‡è³‡æ–™é€²è¡Œå°ˆæ¥­æ¸…æ´—
"""

import re
import emoji
import spacy
from spacy.tokens import Token
from spacy.tokenizer import Tokenizer
from spacy.util import compile_prefix_regex
from typing import Optional, List, Dict, Any
import logging
from pathlib import Path


class TextCleaner:
    """æ–‡æœ¬æ¸…æ´—å™¨ï¼šå°ˆç‚ºTwitteræ¨æ–‡è¨­è¨ˆçš„æ–‡æœ¬é è™•ç†å·¥å…·"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ–‡æœ¬æ¸…æ´—å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«text_cleaningç›¸é—œè¨­å®š
        """
        self.config = config["processing"]["text_cleaning"]
        self.logger = logging.getLogger("data_processor.text_cleaner")
        
        # ç·¨è­¯æ­£å‰‡è¡¨é”å¼æ¨¡å¼
        self._compile_patterns()
        
        # åˆå§‹åŒ–spaCyæ¨¡å‹
        self._setup_spacy()
        
        self.logger.info("TextCleaner åˆå§‹åŒ–å®Œæˆ")
    
    def _compile_patterns(self):
        """ç·¨è­¯æ‰€æœ‰æ­£å‰‡è¡¨é”å¼æ¨¡å¼"""
        # URLå’ŒMentionæ¨¡å¼ (çµ„åˆæ¨¡å¼ä»¥æå‡æ•ˆèƒ½)
        url_pattern = self.config["url_pattern"]
        mention_pattern = self.config["mention_pattern"]
        self.url_mention_pattern = re.compile(
            f"({url_pattern})|({mention_pattern})",
            flags=re.IGNORECASE
        )
        
        # åŠ å¯†è²¨å¹£ç¬¦è™Ÿæ¨¡å¼
        self.crypto_pattern = re.compile(
            self.config["crypto_symbol_pattern"],
            flags=re.IGNORECASE
        )
        
        # Hashtagæ¨¡å¼
        self.hashtag_pattern = re.compile(
            self.config["hashtag_pattern"],
            flags=re.IGNORECASE
        )
        
        # å¤šç©ºç™½å­—å…ƒæ¸…ç†
        self.whitespace_pattern = re.compile(r"\s{2,}")
        
        self.logger.debug("æ­£å‰‡è¡¨é”å¼æ¨¡å¼ç·¨è­¯å®Œæˆ")
    
    def _setup_spacy(self):
        """è¨­å®šspaCy NLPç®¡é“"""
        try:
            # åªè¼‰å…¥å¿…è¦çµ„ä»¶ä»¥æå‡é€Ÿåº¦
            self.nlp = spacy.load(
                "en_core_web_sm",
                disable=["parser", "ner", "textcat"]
            )
            
            # è¨­å®šè‡ªè¨‚tokenizer
            self._setup_custom_tokenizer()
            
            # æ·»åŠ è‡ªè¨‚åœç”¨è©
            self._add_custom_stopwords()
            
            self.logger.debug("spaCyæ¨¡å‹è¼‰å…¥å®Œæˆ")
            
        except OSError:
            self.logger.error("spaCyæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹åŸ·è¡Œ: python -m spacy download en_core_web_sm")
            raise
    
    def _setup_custom_tokenizer(self):
        """è¨­å®šè‡ªè¨‚tokenizerä»¥æ­£ç¢ºè™•ç†åŠ å¯†è²¨å¹£ç¬¦è™Ÿå’Œé€£å­—ç¬¦"""
        # åœ¨åŸæœ‰prefixä¸­æ’é™¤$ç¬¦è™Ÿ
        modified_prefixes = [
            p for p in self.nlp.Defaults.prefixes if p != r'\$'
        ]
        self.nlp.tokenizer.prefix_search = compile_prefix_regex(modified_prefixes).search
        
        # è‡ªè¨‚tokenizerä»¥ä¿ç•™ç‰¹æ®Šç¬¦è™Ÿ
        def custom_tokenizer(nlp):
            # ç‰¹æ®Štokenæ¨¡å¼ï¼šåŠ å¯†è²¨å¹£ç¬¦è™Ÿå’Œé€£å­—ç¬¦è©
            special_patterns = [
                self.crypto_pattern,  # $BTC, $ETHç­‰
                re.compile(r'[A-Za-z]+-[0-9A-Za-z]+'),  # Layer-2, Web3-basedç­‰
                re.compile(r':[a-zA-Z_]+:')  # emojiæè¿° :rocket:
            ]
            
            def special_token_match(text):
                for pattern in special_patterns:
                    match = pattern.match(text)
                    if match:
                        return match
                return None
            
            # ä¿®æ”¹infixæ¨¡å¼ï¼Œä¸åœ¨é€£å­—ç¬¦è©ä¸­åˆ†å‰²
            infix_re = re.compile(r'''[~]''')  # ç§»é™¤é€£å­—ç¬¦åˆ†å‰²
            
            return Tokenizer(
                nlp.vocab,
                prefix_search=nlp.tokenizer.prefix_search,
                suffix_search=nlp.tokenizer.suffix_search,
                infix_finditer=infix_re.finditer,
                token_match=special_token_match
            )
        
        self.nlp.tokenizer = custom_tokenizer(self.nlp)
        self.logger.debug("è‡ªè¨‚tokenizerè¨­å®šå®Œæˆ")
    
    def _add_custom_stopwords(self):
        """æ·»åŠ è‡ªè¨‚åœç”¨è©"""
        extra_stopwords = set(self.config.get("extra_stopwords", []))
        
        for word in extra_stopwords:
            self.nlp.vocab[word].is_stop = True
        
        self.logger.debug(f"æ·»åŠ  {len(extra_stopwords)} å€‹è‡ªè¨‚åœç”¨è©")
    
    # ==================== å…­å€‹é è™•ç†æ­¥é©Ÿ ====================
    
    def step1_normalize_and_filter_retweets(self, text: str) -> Optional[str]:
        """
        æ­¥é©Ÿ1: è½‰å°å¯«ä¸¦éæ¿¾è½‰æ¨
        
        Args:
            text: åŸå§‹æ¨æ–‡æ–‡æœ¬
            
        Returns:
            è™•ç†å¾Œçš„æ–‡æœ¬ï¼Œè‹¥ç‚ºè½‰æ¨å‰‡è¿”å›None
        """
        if not text or not isinstance(text, str):
            return None
            
        # æª¢æŸ¥æ˜¯å¦ç‚ºè½‰æ¨
        if self.config.get("remove_retweets", True):
            if text.strip().startswith("RT ") or text.strip().startswith("RT@"):
                return None
        
        # è½‰æ›ç‚ºå°å¯«
        return text.lower().strip()
    
    def step2_remove_url_mention_preserve_hashtag(self, text: str) -> str:
        """
        æ­¥é©Ÿ2: ç§»é™¤URLå’ŒMentionï¼Œä¿ç•™Hashtagæ–‡å­—
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            æ¸…ç†å¾Œçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # é¦–å…ˆè™•ç†hashtagï¼šç§»é™¤#è™Ÿä½†ä¿ç•™æ–‡å­—
        if self.config.get("preserve_hashtags", True):
            text = re.sub(r"#([a-zA-Z0-9_]+)", r" \1", text)
        
        # ç§»é™¤URLå’ŒMention (æ›´å¼·å¥çš„æ¨¡å¼)
        # URLæ¨¡å¼ï¼šåŒ¹é…http/https/www/pic.twitter.com
        text = re.sub(r"(https?://\S+)|(www\.\S+)|(pic\.twitter\.com/\S+)", "", text, flags=re.IGNORECASE)
        # Mentionæ¨¡å¼ï¼šåŒ¹é…@username
        text = re.sub(r"@\w+", "", text)
        
        # æ¸…ç†å¤šé¤˜ç©ºç™½
        text = self.whitespace_pattern.sub(" ", text).strip()
        
        return text
    
    def step3_convert_emoji(self, text: str) -> str:
        """
        æ­¥é©Ÿ3: å°‡Emojiè½‰æ›ç‚ºæ–‡å­—æè¿°
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            è½‰æ›å¾Œçš„æ–‡æœ¬
        """
        if not text or not self.config.get("convert_emojis", True):
            return text
        
        # ä½¿ç”¨emoji.demojizeè½‰æ›ï¼Œçµ±ä¸€æ ¼å¼
        converted = emoji.demojize(text, language="en", delimiters=(":", ":"))
        # ç¢ºä¿emojiæè¿°å‰å¾Œæœ‰å–®å€‹ç©ºæ ¼
        converted = re.sub(r"\s*(:[\w_]+:)\s*", r" \1 ", converted)
        # æ¸…ç†å¤šé¤˜ç©ºç™½
        converted = re.sub(r"\s+", " ", converted).strip()
        
        return converted
    
    def step4_tokenize_with_crypto_symbols(self, text: str) -> List[str]:
        """
        æ­¥é©Ÿ4: è‡ªè¨‚æ–·è©ï¼Œä¿æŒåŠ å¯†è²¨å¹£ç¬¦è™Ÿå®Œæ•´
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            
        Returns:
            Tokenåˆ—è¡¨
        """
        if not text:
            return []
        
        try:
            doc = self.nlp(text)
            tokens = [token.text for token in doc]
            return tokens
        except Exception as e:
            self.logger.warning(f"æ–·è©è™•ç†å¤±æ•—: {e}")
            # å‚™ç”¨æ–¹æ¡ˆï¼šç°¡å–®ç©ºæ ¼åˆ†å‰²
            return text.split()
    
    def step5_apply_stopwords(self, tokens: List[str]) -> List[str]:
        """
        æ­¥é©Ÿ5: ç§»é™¤åœç”¨è©
        
        Args:
            tokens: Tokenåˆ—è¡¨
            
        Returns:
            éæ¿¾å¾Œçš„Tokenåˆ—è¡¨
        """
        if not tokens:
            return []
        
        filtered_tokens = []
        
        for token in tokens:
            # æª¢æŸ¥æ˜¯å¦ç‚ºåœç”¨è©
            spacy_token = self.nlp.vocab[token.lower()]
            
            # ä¿ç•™ç‰¹æ®Šç¬¦è™Ÿï¼ˆåŠ å¯†è²¨å¹£ç¬¦è™Ÿã€emojiæè¿°ï¼‰
            is_crypto_symbol = self.crypto_pattern.match(token)
            is_emoji_desc = token.startswith(":") and token.endswith(":")
            
            if not spacy_token.is_stop or is_crypto_symbol or is_emoji_desc:
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def step6_lemmatize(self, tokens: List[str]) -> List[str]:
        """
        æ­¥é©Ÿ6: è©å½¢é‚„åŸ
        
        Args:
            tokens: Tokenåˆ—è¡¨
            
        Returns:
            é‚„åŸå¾Œçš„è©æ ¹åˆ—è¡¨
        """
        if not tokens or not self.config.get("apply_lemmatization", True):
            return tokens
        
        try:
            # é‡çµ„æ–‡æœ¬é€²è¡Œè©å½¢é‚„åŸ
            text = " ".join(tokens)
            doc = self.nlp(text)
            
            lemmatized = []
            for token in doc:
                # ä¿ç•™ç‰¹æ®Šç¬¦è™Ÿå’Œemojiæè¿°ä¸é€²è¡Œé‚„åŸ
                if (self.crypto_pattern.match(token.text) or 
                    (token.text.startswith(":") and token.text.endswith(":")) or
                    not token.is_alpha):
                    lemmatized.append(token.text)
                else:
                    lemmatized.append(token.lemma_)
            
            return lemmatized
            
        except Exception as e:
            self.logger.warning(f"è©å½¢é‚„åŸå¤±æ•—: {e}")
            return tokens
    
    # ==================== æ•´åˆè™•ç†å‡½æ•¸ ====================
    
    def clean_text(self, text: str) -> Optional[Dict[str, Any]]:
        """
        å®Œæ•´çš„æ–‡æœ¬æ¸…æ´—æµç¨‹
        
        Args:
            text: åŸå§‹æ¨æ–‡æ–‡æœ¬
            
        Returns:
            æ¸…æ´—çµæœå­—å…¸ï¼ŒåŒ…å«å„éšæ®µè™•ç†çµæœï¼Œå¤±æ•—å‰‡è¿”å›None
        """
        if not text or not isinstance(text, str):
            return None
        
        try:
            # æ­¥é©Ÿ1: æ­£è¦åŒ–å’Œè½‰æ¨éæ¿¾
            step1_result = self.step1_normalize_and_filter_retweets(text)
            if step1_result is None:
                return None  # è½‰æ¨è¢«éæ¿¾
            
            # æ­¥é©Ÿ2: URL/Mentionç§»é™¤ï¼ŒHashtagä¿ç•™
            step2_result = self.step2_remove_url_mention_preserve_hashtag(step1_result)
            
            # æ­¥é©Ÿ3: Emojiè½‰æ›
            step3_result = self.step3_convert_emoji(step2_result)
            
            # æ­¥é©Ÿ4: è‡ªè¨‚æ–·è©
            step4_result = self.step4_tokenize_with_crypto_symbols(step3_result)
            
            # æ­¥é©Ÿ5: åœç”¨è©éæ¿¾
            step5_result = self.step5_apply_stopwords(step4_result)
            
            # æ­¥é©Ÿ6: è©å½¢é‚„åŸ
            step6_result = self.step6_lemmatize(step5_result)
            
            # å“è³ªæª¢æŸ¥
            if len(step6_result) == 0 and not self.config.get("allow_empty_after_cleaning", False):
                return None
            
            # çµ„è£çµæœ
            result = {
                "original_text": text,
                "normalized_text": step1_result,
                "cleaned_text": step3_result,  # ä¿ç•™emojiè½‰æ›å¾Œçš„å¯è®€æ–‡æœ¬
                "tokens": step4_result,
                "filtered_tokens": step5_result,
                "final_tokens": step6_result,
                "token_count": len(step6_result),
                "processing_steps": {
                    "retweet_filtered": step1_result != text.lower(),
                    "urls_mentions_removed": step2_result != step1_result,
                    "emojis_converted": step3_result != step2_result,
                    "stopwords_removed": len(step5_result) < len(step4_result),
                    "lemmatized": step6_result != step5_result
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬æ¸…æ´—å¤±æ•—: {text[:100]}... éŒ¯èª¤: {e}")
            return None
    
    def batch_clean(self, texts: List[str]) -> List[Optional[Dict[str, Any]]]:
        """
        æ‰¹æ¬¡è™•ç†æ–‡æœ¬æ¸…æ´—
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æ¸…æ´—çµæœåˆ—è¡¨
        """
        results = []
        
        for text in texts:
            result = self.clean_text(text)
            results.append(result)
        
        return results
    
    # ==================== é©—è­‰å’Œçµ±è¨ˆ ====================
    
    def validate_setup(self) -> Dict[str, bool]:
        """é©—è­‰æ¸…æ´—å™¨è¨­å®šæ˜¯å¦æ­£ç¢º"""
        test_cases = [
            ("$BTC is going to the moon! ğŸš€ #crypto", True),   # æ‡‰è©²æˆåŠŸè™•ç†
            ("RT @user: This is a retweet", False),            # æ‡‰è©²è¢«éæ¿¾ (è¿”å›None)
            ("Check out https://example.com @mention #DeFi", True),  # æ‡‰è©²æˆåŠŸè™•ç†
            ("Layer-2 solutions are amazing ğŸ’¯", True)        # æ‡‰è©²æˆåŠŸè™•ç†
        ]
        
        results = {}
        
        try:
            # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
            for i, (test_text, should_succeed) in enumerate(test_cases, 1):
                result = self.clean_text(test_text)
                actual_success = result is not None
                results[f"test_case_{i}"] = actual_success == should_succeed
            
            # æ¸¬è©¦ç‰¹æ®Štoken
            tokens = self.step4_tokenize_with_crypto_symbols("$BTC Layer-2 test")
            results["crypto_symbol_preserved"] = "$BTC" in tokens
            results["hyphen_preserved"] = "Layer-2" in tokens
            
            self.logger.info("è¨­å®šé©—è­‰å®Œæˆ")
            return results
            
        except Exception as e:
            self.logger.error(f"è¨­å®šé©—è­‰å¤±æ•—: {e}")
            return {"validation_failed": True, "error": str(e)}
    
    def get_stats(self) -> Dict[str, Any]:
        """å–å¾—æ¸…æ´—å™¨çµ±è¨ˆè³‡è¨Š"""
        return {
            "config": self.config,
            "spacy_model": self.nlp.meta["name"] if hasattr(self.nlp, 'meta') else "unknown",
            "custom_stopwords_count": len(self.config.get("extra_stopwords", [])),
            "patterns_compiled": {
                "url_mention": bool(self.url_mention_pattern),
                "crypto_symbol": bool(self.crypto_pattern),
                "hashtag": bool(self.hashtag_pattern)
            }
        } 