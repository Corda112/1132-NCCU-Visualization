"""
ä¸»è¦è³‡æ–™è™•ç†å™¨
æ•´åˆæ–‡æœ¬æ¸…æ´—ã€å“è³ªç›£æŽ§å’Œæª”æ¡ˆè™•ç†åŠŸèƒ½çš„æ ¸å¿ƒè™•ç†å™¨
"""

import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Generator
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import orjson
import uuid
from datetime import datetime

from .text_cleaner import TextCleaner
from .quality_monitor import QualityMonitor
from .utils import (
    safe_json_load, safe_json_save, generate_uuid_from_tweet_id,
    validate_file_structure, get_file_stats
)


class DataProcessor:
    """ä¸»è¦è³‡æ–™è™•ç†å™¨ï¼šè² è²¬å”èª¿æ•´å€‹è³‡æ–™è™•ç†æµç¨‹"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è³‡æ–™è™•ç†å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
        """
        self.config = config
        self.processing_config = config["processing"]
        self.paths_config = self.processing_config["paths"]
        self.perf_config = self.processing_config["performance"]
        
        self.logger = logging.getLogger("data_processor.main")
        
        # åˆå§‹åŒ–å­æ¨¡çµ„
        self.text_cleaner = TextCleaner(config)
        self.quality_monitor = QualityMonitor(config)
        
        # è¨­å®šè·¯å¾‘
        self._setup_directories()
        
        self.logger.info("DataProcessor åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_directories(self):
        """è¨­å®šå¿…è¦çš„ç›®éŒ„çµæ§‹"""
        directories = [
            self.paths_config["processed_data"],
            self.paths_config["logs"],
            self.paths_config["temp"]
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
        
        self.logger.debug("ç›®éŒ„çµæ§‹è¨­å®šå®Œæˆ")
    
    def process_single_file(self, 
                          input_file: Path, 
                          output_file: Optional[Path] = None,
                          preserve_metadata: bool = True) -> Dict[str, Any]:
        """
        è™•ç†å–®å€‹JSONæª”æ¡ˆ
        
        Args:
            input_file: è¼¸å…¥æª”æ¡ˆè·¯å¾‘
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
            preserve_metadata: æ˜¯å¦ä¿ç•™åŽŸå§‹metadata
            
        Returns:
            è™•ç†çµæžœçµ±è¨ˆ
        """
        self.logger.info(f"é–‹å§‹è™•ç†æª”æ¡ˆ: {input_file}")
        
        # è¼‰å…¥è³‡æ–™
        raw_data = safe_json_load(input_file)
        if raw_data is None:
            return {"error": f"ç„¡æ³•è¼‰å…¥æª”æ¡ˆ: {input_file}"}
        
        # é©—è­‰æª”æ¡ˆçµæ§‹
        required_fields = ["id", "text"]
        if not validate_file_structure(input_file, required_fields):
            return {"error": f"æª”æ¡ˆçµæ§‹ä¸ç¬¦åˆè¦æ±‚: {input_file}"}
        
        # è™•ç†è³‡æ–™
        start_time = time.time()
        processed_data = self._process_data_chunks(raw_data, preserve_metadata)
        processing_time = time.time() - start_time
        
        # å“è³ªç›£æŽ§
        raw_texts = [item["text"] for item in raw_data]
        cleaned_results = [item.get("cleaned_data") for item in processed_data 
                          if item.get("cleaned_data") is not None]
        
        quality_report = self.quality_monitor.monitor_batch(
            raw_texts, cleaned_results, processing_time
        )
        
        # å„²å­˜çµæžœ
        if output_file is None:
            output_file = self._generate_output_path(input_file)
        
        success = safe_json_save(processed_data, output_file)
        
        # çµ„è£çµæžœçµ±è¨ˆ
        result_stats = {
            "input_file": str(input_file),
            "output_file": str(output_file) if success else None,
            "input_count": len(raw_data),
            "output_count": len(processed_data),
            "processing_time": processing_time,
            "success": success,
            "quality_report": quality_report,
            "file_stats": {
                "input": get_file_stats(input_file),
                "output": get_file_stats(output_file) if success else None
            }
        }
        
        self.logger.info(
            f"æª”æ¡ˆè™•ç†å®Œæˆ: {input_file} -> {output_file} "
            f"({len(raw_data)} -> {len(processed_data)} æ¢)"
        )
        
        return result_stats
    
    def process_directory(self, 
                         input_dir: Optional[Path] = None,
                         output_dir: Optional[Path] = None,
                         file_pattern: str = "*.json") -> Dict[str, Any]:
        """
        æ‰¹æ¬¡è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
        
        Args:
            input_dir: è¼¸å…¥ç›®éŒ„ï¼ˆé è¨­ä½¿ç”¨configä¸­çš„raw_dataï¼‰
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ä½¿ç”¨configä¸­çš„processed_dataï¼‰
            file_pattern: æª”æ¡ˆåŒ¹é…æ¨¡å¼
            
        Returns:
            æ•´é«”è™•ç†çµ±è¨ˆ
        """
        if input_dir is None:
            input_dir = Path(self.paths_config["raw_data"])
        if output_dir is None:
            output_dir = Path(self.paths_config["processed_data"])
        
        self.logger.info(f"é–‹å§‹æ‰¹æ¬¡è™•ç†ç›®éŒ„: {input_dir}")
        
        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æª”æ¡ˆ
        input_files = list(input_dir.glob(file_pattern))
        if not input_files:
            return {"error": f"æœªæ‰¾åˆ°åŒ¹é…çš„æª”æ¡ˆ: {input_dir}/{file_pattern}"}
        
        self.logger.info(f"æ‰¾åˆ° {len(input_files)} å€‹æª”æ¡ˆå¾…è™•ç†")
        
        # é‡ç½®å“è³ªç›£æŽ§çµ±è¨ˆ
        self.quality_monitor.reset_stats()
        
        # è™•ç†æ‰€æœ‰æª”æ¡ˆ
        all_results = []
        failed_files = []
        
        # ä½¿ç”¨é€²åº¦æ¢é¡¯ç¤ºè™•ç†é€²åº¦
        with tqdm(total=len(input_files), desc="è™•ç†æª”æ¡ˆ", unit="file") as pbar:
            
            if self.perf_config["max_workers"] > 1:
                # å¤šåŸ·è¡Œç·’è™•ç†
                all_results = self._process_files_parallel(
                    input_files, output_dir, pbar, failed_files
                )
            else:
                # å–®åŸ·è¡Œç·’è™•ç†
                all_results = self._process_files_sequential(
                    input_files, output_dir, pbar, failed_files
                )
        
        # ç”Ÿæˆæ•´é«”çµ±è¨ˆ
        overall_stats = self._generate_overall_stats(all_results, failed_files)
        
        # å„²å­˜å“è³ªå ±å‘Š
        quality_report_path = output_dir / f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.quality_monitor.save_report(quality_report_path)
        
        # é¡¯ç¤ºæ‘˜è¦
        self.quality_monitor.print_summary()
        
        self.logger.info(f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼Œå…±è™•ç† {len(input_files)} å€‹æª”æ¡ˆ")
        
        return overall_stats
    
    def _process_files_parallel(self, 
                               input_files: List[Path], 
                               output_dir: Path,
                               pbar: tqdm,
                               failed_files: List[str]) -> List[Dict[str, Any]]:
        """å¤šåŸ·è¡Œç·’è™•ç†æª”æ¡ˆ"""
        all_results = []
        
        with ThreadPoolExecutor(max_workers=self.perf_config["max_workers"]) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_file = {
                executor.submit(
                    self.process_single_file,
                    input_file,
                    output_dir / f"cleaned_{input_file.name}"
                ): input_file
                for input_file in input_files
            }
            
            # æ”¶é›†çµæžœ
            for future in as_completed(future_to_file):
                input_file = future_to_file[future]
                try:
                    result = future.result()
                    if "error" in result:
                        failed_files.append(str(input_file))
                        self.logger.error(f"è™•ç†å¤±æ•—: {input_file} - {result['error']}")
                    else:
                        all_results.append(result)
                except Exception as e:
                    failed_files.append(str(input_file))
                    self.logger.error(f"è™•ç†ç•°å¸¸: {input_file} - {e}")
                finally:
                    pbar.update(1)
        
        return all_results
    
    def _process_files_sequential(self, 
                                 input_files: List[Path], 
                                 output_dir: Path,
                                 pbar: tqdm,
                                 failed_files: List[str]) -> List[Dict[str, Any]]:
        """å–®åŸ·è¡Œç·’è™•ç†æª”æ¡ˆ"""
        all_results = []
        
        for input_file in input_files:
            try:
                output_file = output_dir / f"cleaned_{input_file.name}"
                result = self.process_single_file(input_file, output_file)
                
                if "error" in result:
                    failed_files.append(str(input_file))
                    self.logger.error(f"è™•ç†å¤±æ•—: {input_file} - {result['error']}")
                else:
                    all_results.append(result)
                    
            except Exception as e:
                failed_files.append(str(input_file))
                self.logger.error(f"è™•ç†ç•°å¸¸: {input_file} - {e}")
            finally:
                pbar.update(1)
        
        return all_results
    
    def _process_data_chunks(self, 
                           raw_data: List[Dict[str, Any]], 
                           preserve_metadata: bool) -> List[Dict[str, Any]]:
        """åˆ†å¡Šè™•ç†è³‡æ–™ä»¥æå‡æ•ˆèƒ½"""
        chunk_size = self.perf_config["chunk_size"]
        processed_data = []
        
        for i in range(0, len(raw_data), chunk_size):
            chunk = raw_data[i:i + chunk_size]
            chunk_results = self._process_chunk(chunk, preserve_metadata)
            processed_data.extend(chunk_results)
        
        return processed_data
    
    def _process_chunk(self, 
                      chunk: List[Dict[str, Any]], 
                      preserve_metadata: bool) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹è³‡æ–™å¡Š"""
        processed_chunk = []
        
        for tweet_data in chunk:
            try:
                # æå–æ–‡æœ¬
                text = tweet_data.get("text", "")
                tweet_id = tweet_data.get("id", "")
                
                # æ–‡æœ¬æ¸…æ´—
                cleaned_result = self.text_cleaner.clean_text(text)
                
                # çµ„è£è¼¸å‡ºè³‡æ–™
                output_item = {
                    "uuid": generate_uuid_from_tweet_id(tweet_id),
                    "original_tweet_id": tweet_id,
                    "processing_timestamp": datetime.now().isoformat(),
                }
                
                # æ·»åŠ æ¸…æ´—çµæžœ
                if cleaned_result is not None:
                    output_item["cleaned_data"] = cleaned_result
                    output_item["status"] = "success"
                else:
                    output_item["status"] = "filtered_or_failed"
                    output_item["reason"] = "retweet_filtered_or_processing_failed"
                
                # ä¿ç•™åŽŸå§‹metadataï¼ˆå¯é¸ï¼‰
                if preserve_metadata:
                    output_item["original_metadata"] = {
                        key: value for key, value in tweet_data.items()
                        if key not in ["text"]  # é¿å…é‡è¤‡å­˜å„²text
                    }
                
                processed_chunk.append(output_item)
                
            except Exception as e:
                self.logger.warning(f"è™•ç†æŽ¨æ–‡å¤±æ•— {tweet_data.get('id', 'unknown')}: {e}")
                # æ·»åŠ å¤±æ•—è¨˜éŒ„
                processed_chunk.append({
                    "uuid": generate_uuid_from_tweet_id(tweet_data.get("id", "unknown")),
                    "original_tweet_id": tweet_data.get("id", ""),
                    "status": "error",
                    "error": str(e),
                    "processing_timestamp": datetime.now().isoformat(),
                })
        
        return processed_chunk
    
    def _generate_output_path(self, input_file: Path) -> Path:
        """ç‚ºè¼¸å…¥æª”æ¡ˆç”Ÿæˆå°æ‡‰çš„è¼¸å‡ºè·¯å¾‘"""
        output_dir = Path(self.paths_config["processed_data"])
        return output_dir / f"cleaned_{input_file.name}"
    
    def _generate_overall_stats(self, 
                               all_results: List[Dict[str, Any]], 
                               failed_files: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´é«”è™•ç†çµ±è¨ˆ"""
        total_files = len(all_results) + len(failed_files)
        successful_files = len(all_results)
        
        total_input = sum(r["input_count"] for r in all_results)
        total_output = sum(r["output_count"] for r in all_results)
        total_time = sum(r["processing_time"] for r in all_results)
        
        overall_stats = {
            "processing_summary": {
                "total_files": total_files,
                "successful_files": successful_files,
                "failed_files": len(failed_files),
                "success_rate": successful_files / total_files if total_files > 0 else 0,
                "total_tweets_input": total_input,
                "total_tweets_output": total_output,
                "total_processing_time": total_time,
                "avg_processing_time_per_file": total_time / successful_files if successful_files > 0 else 0
            },
            "failed_files": failed_files,
            "file_results": all_results,
            "quality_summary": self.quality_monitor.generate_report(),
            "processing_config": self.processing_config,
            "generated_at": datetime.now().isoformat()
        }
        
        return overall_stats
    
    # ==================== ä¾¿åˆ©æ–¹æ³• ====================
    
    def quick_process_all(self) -> Dict[str, Any]:
        """å¿«é€Ÿè™•ç†æ‰€æœ‰rawè³‡æ–™æª”æ¡ˆ"""
        return self.process_directory()
    
    def process_specific_files(self, file_names: List[str]) -> Dict[str, Any]:
        """è™•ç†æŒ‡å®šçš„æª”æ¡ˆåˆ—è¡¨"""
        input_dir = Path(self.paths_config["raw_data"])
        output_dir = Path(self.paths_config["processed_data"])
        
        input_files = [input_dir / name for name in file_names]
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        existing_files = [f for f in input_files if f.exists()]
        missing_files = [f for f in input_files if not f.exists()]
        
        if missing_files:
            self.logger.warning(f"ä»¥ä¸‹æª”æ¡ˆä¸å­˜åœ¨: {[str(f) for f in missing_files]}")
        
        if not existing_files:
            return {"error": "æ²’æœ‰æ‰¾åˆ°ä»»ä½•æŒ‡å®šçš„æª”æ¡ˆ"}
        
        # é‡ç½®å“è³ªç›£æŽ§
        self.quality_monitor.reset_stats()
        
        # è™•ç†å­˜åœ¨çš„æª”æ¡ˆ
        all_results = []
        failed_files = []
        
        with tqdm(total=len(existing_files), desc="è™•ç†æŒ‡å®šæª”æ¡ˆ", unit="file") as pbar:
            for input_file in existing_files:
                try:
                    output_file = output_dir / f"cleaned_{input_file.name}"
                    result = self.process_single_file(input_file, output_file)
                    
                    if "error" in result:
                        failed_files.append(str(input_file))
                    else:
                        all_results.append(result)
                        
                except Exception as e:
                    failed_files.append(str(input_file))
                    self.logger.error(f"è™•ç†ç•°å¸¸: {input_file} - {e}")
                finally:
                    pbar.update(1)
        
        return self._generate_overall_stats(all_results, failed_files)
    
    def validate_processing_setup(self) -> Dict[str, Any]:
        """é©—è­‰è™•ç†å™¨è¨­å®š"""
        validation_results = {
            "directories": {},
            "text_cleaner": {},
            "config": {},
            "sample_processing": {}
        }
        
        # æª¢æŸ¥ç›®éŒ„
        for path_name, path_value in self.paths_config.items():
            path_obj = Path(path_value)
            validation_results["directories"][path_name] = {
                "exists": path_obj.exists(),
                "is_directory": path_obj.is_dir() if path_obj.exists() else False,
                "writable": path_obj.exists() and path_obj.is_dir()
            }
        
        # æª¢æŸ¥æ–‡æœ¬æ¸…æ´—å™¨
        validation_results["text_cleaner"] = self.text_cleaner.validate_setup()
        
        # æª¢æŸ¥é…ç½®
        validation_results["config"] = {
            "max_workers": self.perf_config["max_workers"] > 0,
            "batch_size": self.perf_config["batch_size"] > 0,
            "chunk_size": self.perf_config["chunk_size"] > 0
        }
        
        # æ¸¬è©¦æ¨£æœ¬è™•ç†
        sample_tweet = {
            "id": "test123",
            "text": "$BTC is going to the moon! ðŸš€ #crypto https://example.com @user",
            "createdAt": "2024-01-01"
        }
        
        try:
            result = self._process_chunk([sample_tweet], preserve_metadata=True)
            validation_results["sample_processing"] = {
                "success": len(result) > 0,
                "has_cleaned_data": result[0].get("cleaned_data") is not None if result else False
            }
        except Exception as e:
            validation_results["sample_processing"] = {
                "success": False,
                "error": str(e)
            }
        
        return validation_results 